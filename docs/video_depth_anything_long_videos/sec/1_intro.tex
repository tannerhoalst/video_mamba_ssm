\section{Introduction}
\label{sec:intro}

Recently, monocular depth estimation (MDE) has made significant progress, as evidenced by advances in depth foundation models~\cite{depth_anything_v1,depth_anything_v2,marigold,birkl2023midas}. For example, Depth Anything V2~\cite{depth_anything_v2} demonstrates a strong generalization ability in producing depth predictions with rich details in various scenarios while being computationally efficient. However, these models have a major limitation: they are mainly designed for static images and suffer from flickering and motion blur in videos. This limitation restricts their applications in robotics~\cite{dong2022towards}, augmented reality~\cite{holynski2018fast}, and advanced video editing~\cite{zhang2023controlvideo,peng2024controlnext}, which requires temporally consistent depth.

Extensive efforts are being made to address this problem. Early work~\cite{kopf2021robust,luo2020consistent,zhang2021consistent} often relies on test-time optimization to tune a pretrained monocular depth model with sophisticated geometry constraints. Given the heavy overhead at inference time of these methods, recent work mainly focuses on feedforward models and can be categorized into two approaches. The first approach~\cite{wang2023neural} involves designing a plug-and-play module to augment monocular depth model predictions with temporal consistency. The training of such a module is highly dependent on optical flow~\cite{xu2022gmflow} or camera poses~\cite{schonberger2016structure} for consistency constraints, making the module susceptible to corresponding errors. The second approach~\cite{chronodepth,hu2024depthcrafter,depthanyvideo} repurposes pre-trained video diffusion models~\cite{blattmann2023stable} into video-to-depth models. These methods excel at producing fine-grained details, but are computationally inefficient, cannot leverage existing depth foundation models, and can only handle videos with limited length. 

Then, a natural question arises: \emph{Is it possible to have a model that can perfectly inherit the capabilities of existing foundation models while achieving temporal stability for arbitrarily long videos}? In this paper, we show that the answer is YES by developing \textbf{\name} based on Depth Anything V2, without sacrificing its generalization ability, richness in details, or computational efficiency. This target is achieved without introducing any geometric priors or video generation priors.

Specifically, we first design a lightweight spatial-temporal head (STH) to replace the DPT head~\cite{ranftl2021vision} and enable temporal information interactions.
STH includes four temporal attention layers, applied along the temporal dimension for each spatial position. Introducing temporal attention only in the head prevents the learned representation from being corrupted by the limited video data. Then, we propose a \textit{temporal gradient matching loss} to constrain depth prediction gradients along the temporal dimension to match those calculated from the ground truth. This loss function is jointly optimized with the scale-shift-invariant loss and spatial gradient matching loss~\cite{birkl2023midas,depth_anything_v1}. Despite its simplicity, it can effectively boost the model's temporal consistency. Third, to maintain the original capabilities of the model, we train it jointly on 730K video frames with depth annotations using supervised learning, and on 0.62 million unlabeled images using self-training, similar to Depth Anything V2~\cite{depth_anything_v2}. To handle super-long videos at inference time, we developed a novel segment-wise processing strategy. Each new segment is concatenated with eight overlapping frames and two key frames from the previous video clips, forming a total of 32 frames. Then, the overlapping frames will be progressively interpolated between the two consecutive windows to ensure smoothness. 

We compare our model with baselines on five datasets for zero-shot video depth estimation. Our model achieves state-of-the-art (SOTA) results on four of the datasets in terms of spatial accuracy and outperforms all baselines on all datasets in terms of temporal consistency. Not only can our model produce depth outputs visually comparable to video-diffusion-based methods, but it is also significantly more computationally efficient. For the first time, we can estimate consistent depth for videos over several minutes (see \cref{fig:teaser}). Additionally, we tested our model for zero-shot image depth estimation on five datasets, noting only a marginal performance drop on one dataset compared to Depth Anything V2. As shown in \cref{fig:teaser} (right), our model achieves the best performance in all three aspects: spatial accuracy, temporal consistency, and computational efficiency.

Our contributions are summarized as follows:
\begin{itemize} 
\item We develop a novel method to transform Depth Anything into \name for depth estimation in arbitrarily long videos.
\item We propose a simple yet effective loss function that enforces temporal consistency constraints without introducing geometric or generative priors.
\item Our model not only sets new SOTA (both spatially and temporally) in video depth estimation but is also the most computationally efficient.
\end{itemize}