\section{Related Work}
\label{sec:related_work}
{\bf Monocular depth estimation.} Early monocular depth estimation ~\cite{eigen2014depth,fu2018dorn,bhat2021adabins,yuan2022crf} efforts were primarily trained and tested on in-domain datasets. These models were constrained by the limited diversity of their training data, showing bad generalization for zero-shot application. Subsequently, MiDaS~\cite{birkl2023midas} introduced multi-dataset mixed training using an affine-invariant alignment method, significantly enhancing model generalization. However, due to limitations in the backbone model's performance and noise in the labeled depth data, the resulting depth maps lacked fine details. Following MiDaS~\cite{birkl2023midas}, monocular depth estimation models have generally bifurcated into two categories: relative depth models that estimate affine-invariant depth (e.g., DPT~\cite{ranftl2021vision}, DepthAnything~\cite{depth_anything_v1,depth_anything_v2}, Marigold~\cite{marigold}) and metric depth models that estimate depth with an absolute scale (e.g., ZoeDepth~\cite{bhat2023zoedepth}, Metric3D~\cite{yin2023metric3d}, UniDepth~\cite{piccinelli2024unidepth}). Metric depth models require training with metric depth data that includes camera parameters~\cite{yin2023metric3d}, thus their available training data is more limited compared to affine-invariant depth models, resulting in poorer generalization. Recently, Depth Anything V2~\cite{depth_anything_v2} leveraged the Dinov2~\cite{oquab2023dinov2} pre-trained backbone to train an affine-invariant large-scale model using synthetic data with high-detail fidelity. This large model was then used as a teacher to distill smaller models on 62 million unlabeled datasets~\cite{depth_anything_v1}, achieving SOTA performance in both generalization and geometric details. However, since Depth Anything V2~\cite{depth_anything_v2} was trained exclusively on static images, thus lacks temporal consistency.
% in videos~\cite{hu2024depthcrafter}.

{\bf Consistent video depth estimation.} The core task of consistent video depth estimation is to obtain temporal consistent and accuracy depth maps. Early methods for video depth relied on test-time training~\cite{luo2020consistent,kopf2021robust,zhang2021consistent}, which were impractical for applications for their low efficiency. In recent years, learning-based models have emerged. Some of these models, such as MAMo~\cite{yasarla2023mamo}, use optical flow~\cite{teed2020raft} to warp features, while others~\cite{sayed2022simplerecon} depends on relative poses between frames to construct cost volumes. However, their performance were suffered from errors of inaccurate optical flow or pose estimation. Additional approaches have attempted to enhance off-the-shelf monocular depth estimation (MDE) models with temporal stability model blocks ~\cite{wang2023neural}. Nevertheless, these efforts have not achieved satisfactory results due to suboptimal model designs and inadequate geometric consistency constraints. Furthermore, video diffusion models such as ChronoDepth~\cite{chronodepth}, DepthCrafter~\cite{hu2024depthcrafter}, and DepthAnyVideo\cite{depthanyvideo} show better details and temporal consistency. But they suffered from slow inference speeds and require extensive video depth training data. Limited by the large memory, these models~\cite{hu2024depthcrafter} were typically tested only within the maximum window length used during training, leading to depth flickering between windows and poor temporal and spatial consistency in long videos.