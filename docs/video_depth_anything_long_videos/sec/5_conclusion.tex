\section{Conclusion}
\label{sec:conclusion}

In this paper, we present a novel method named \name for estimating the depth of the video that is temporally consistent. The model is built on top of Depth Anything V2 and is based on three key components. First, a spatial-temporal head to involve temporal interactions by applying a temporal self-attention layer to feature maps. Second, a simple temporal gradient matching loss function is used to enforce temporal consistency. Third, to enable long-video depth estimation, a novel keyframe-based strategy is developed for segment-wise inference along with a depth stitching method. Extensive experiments show that our model achieves state-of-the-art performance in three aspects: spatial accuracy, temporal consistency, and computational efficiency. Consequently, it can produce high-quality depth predictions for videos lasting several minutes.
