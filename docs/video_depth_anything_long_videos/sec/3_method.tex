\section{Video Depth Anything}
\label{sec:method}

In this section, we introduce Video Depth Anything, a feed-forward video transformer model to efficiently estimate temporally consistent video depth. We adopt the affine-invariant depth, but share the same scale and shift across the entire video. The pipeline of our method is shown in Fig.~\ref{fig:overview_head}. Our model is built upon Depth Anything V2 with an additional temporal module and video dataset training (Sec.~\ref{subsec::video}). A novel loss to enfoce temporal consistency is proposed in Sec.~\ref{subsec::loss}. Finally, a strategy combined with overlapping frames and key frames is presented to efficiently support super-long video inference (Sect.~\ref{subsec::long}). 

\input{figures/overview_head}
\subsection{Architecture}
\label{subsec::video}

Due to the lack of sufficient video depth data, we start with a pre-trained image depth estimation model, Depth Anything V2, and adopt a joint training strategy using both image and video data.

\noindent{\bf Depth Anything V2 Encoder.} Depth Anything V2~\cite{depth_anything_v2} is the current state-of-the-art monocular depth estimation model, characterized by its high accuracy and generalization capabilities. We use its trained model as our encoder. To reduce training costs and preserve well-learned features, the encoder is frozen during training.

Unlike monocular depth encoders that only accept image input, our training scenario requires the encoder to process simultaneously both video and image data. To extract features from video frames with an image encoder, we collapse the temporal dimension of a video clip into the batch dimension. The input data are denoted as $\mathbf{X} \in \mathbb{R}^{(B \times N) \times C \times H \times W} $, where $B$ represents the batch size, $N$ is the number of frames in the video clip, $N=1$ for the image as input, $C,H,W$ are the number of channels, height, width of the frames, respectively. The encoder takes $\mathbf{X}$ as input to produce a series of intermediate feature maps $\mathbf{F_i} \in \mathbb{R}^{(B \times N) \times (\frac{H}{p} \times \frac{W}{p}) \times C_i }$, $p$ is the patch size of the encoder. Although the image encoder extracts strong visual representations from individual frames, it neglects the temporal information interactions between frames. Thus, the spatiotemporal head is introduced to model the temporal relationship among the frames.


\noindent{\bf Spatiotemporal Head.}
The spatiotemporal head (STH) is built upon the DPT~\cite{ranftl2021vision} head and with the only modification being the insertion of temporal layers to capture temporal information.
%integrates temporal layers to capture temporal information. 
A temporal layer consists of a multi-head self-attention~\cite{vaswani2017attention} model (SA) and a feed-forward network (FFN). When inputting a feature $\mathbf{F_i}$ into the temporal layer, the temporal dimension $N$ is isolated, and self-attention is executed solely along the temporal dimension to facilitate the interaction of temporal features. To capture temporal positional relationships among different frames, we utilize absolute positional embedding to encode temporal positional information from the video sequence.

The spatiotemporal head uniformly samples 4 feature maps from $\mathbf{F_i}$ (including the final features from the encoder, denoted as $\mathbf{F_{4}}$) as inputs, and predicts a depth map $\mathbf{D} \in \mathbb{R}^{H \times W}$. As shown in Figure~\ref{fig:overview_head}, the selected features $\mathbf{F_i}$ are fed into the Reassemble layer to produce a feature pyramid. Then, the features are gradually fused from low resolution to high resolution by the Fusion layer. The Reassemble and Fusion layer are proposed by DPT~\cite{ranftl2021vision}. The final fused high-resolution feature maps are passed through the output layer to produce the depth map $\mathbf{D}$. To reduce the additional computational load, we insert the temporal layer at a few positions with lower feature resolutions. 

\subsection{Temporal Gradient Matching loss}
\label{subsec::loss}

In this section, we start with the Optical Flow Based Warping (OPW) loss, then explore new loss designs and ultimately propose a Temporal Gradient Matching Loss (TGM) that does not rely on optical flow, yet still ensures the temporal consistency of predictions between frames.

\noindent{\bf OPW loss.} To constrain temporal consistency, previous video models such as~\cite{wang2023neural,10.1145/3591106.3592264,Wang_2022} assume that the depths at corresponding positions in adjacent frames, identified through optical flow, are consistent, \textit{e.g.}, the Optical Flow based Warping (OPW) loss proposed in NVDS~\cite{wang2023neural}. OPW loss is computed after obtaining corresponding points on the basis of optical flow and warping. Specifically, for two consecutive depth prediction results, $p_i$ and $p_{i+1}$. $p_{i+1}$ is warped to $\hat{p_i}$ according to the wrapping relationship derived from the optical flow, and then the loss is calculated with:
\vspace{-3mm}
\begin{equation}
  \mathcal{L}_\text{OPW}= \frac {1}{N-1} \sum _ {i=2}^ {N} \parallel p_i - \hat{p_i}\parallel_1,
  \label{eq:opw_loss}
\vspace{-3mm} 
\end{equation}
where $N$ denotes the length of a video window, and $||\cdot||_1$ represents $\ell$1 distance. However, there is a fundamental issue with the OPW loss: the depth of corresponding points is not invariant across adjacent frames. This assumption holds true only when adjacent frames are stationary. For instance, in driving scenario, when a car is moving forward, the distance to static objects in front decreases relative to the car, violating the assumption of $\mathcal{L}_{OPW}$. To address this inherent issue of OPW, we propose a new loss function to constrain the temporal consistency of depth.

\noindent{\bf Temporal gradient matching loss (TGM)}. When calculating the loss, we do not assume that the depth of the corresponding points in adjacent frames remains unchanged. Instead, we posit that the change in depth of corresponding points between adjacent prediction frames should be consistent with the change observed in ground truth. We refer to this discrepancy as a stable error (SE) given by: 
\vspace{-3mm}
\begin{equation}
    \mathcal{L}_\text{SE} = \frac{1}{N-1} \sum_{i=1}^{N-1}\parallel \mid\hat{d_i}-d_{i}\mid - \mid\hat{g_i}-g_{i}\mid \parallel_1.
  \label{eq:stable_loss}
  \vspace{-3mm}
\end{equation}
Here, $d_i,g_i$ are scaled and shifted versions of the predictions and ground truth. $\hat{d_i},\hat{g_i}$ denotes the warped depth from the subsequent frame using optical flow. $\mid\cdot\mid$ is used to represent the absolute values. 

However, generating optical flow incurs additional overhead. To address the dependence on optical flow, we further generalize the above assumption. Specifically, it is not necessary to use the corresponding points obtained from the optical flow. Instead, we directly use the depth at the same coordinate in adjacent frames to calculate the loss. The assumption is that the change in depth at the same image position between adjacent frames should be consistent with that in the ground truth. Since this process is akin to calculating the gradient of values in temporal dimension, we name it Temporal Gradient Matching Loss, as given by
\vspace{-3mm}
\begin{equation}
    \mathcal{L}_\text{TGM} = \frac{1}{N-1} \sum_{i=1}^{N-1}\parallel \mid d_{i+1}-d_{i}\mid - \mid g_{i+1}-g_{i}\mid \parallel_1.
    \vspace{-3mm}
\end{equation}
In practice, we only compute the TGM loss in regions where the change in ground truth depth, \textit{i.e.}, $\mid g_{i + 1} - g_i \mid < 0.05 $. This threshold helps to avoid sudden changes in depth map caused by edges, dynamic objects, and other factors that introduce unsteadiness during training. 

Our total loss to supervise video depth data is as follows:
\vspace{-3mm}
\begin{equation}
    \mathcal{L}_{\text{all}} = \alpha \mathcal{L}_{\text{TGM}} + \beta  \mathcal{L}_{\text{ssi}},
  \label{eq:all_loss}
  \vspace{-3mm}
\end{equation}
where $\mathcal{L}_{ssi}$ is a scale- and shift-invariant loss to supervise single images proposed by MiDaS~\cite{birkl2023midas}. $\alpha$ and $\beta$ are weights to balance spatio-temporal consistency and spatial structure in a single frame. 

\subsection{Inference strategy for super-long sequence}
\label{subsec::long}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/imgs/long_inference-fix.pdf}
    \caption{\textbf{Inference strategy for long videos}. $N$ is the video clip lenght consumed by our model. Each inference video clip is built by $N-T_o-T_k$ future frames, $T_o$ overlapping/adjacent frames, and $T_k$ key frames. The key frames are selected by taking every $\Delta_k$-th frame going backward. Then, the new depth predictions will be scale-shift-aligned to the previous frames based on the $T_k$ overlapping frames. We use $N=32, T_o=8, T_k=2, \Delta_k=12$.}
    \label{fig:long_inference}
    \vspace{-18pt}
\end{figure}
To handle videos of arbitrary length, a straightforward approach is simply to concatenate the model outputs from different video windows. However, this method fails to ensure smooth transitions between windows. A more sophisticated technique entails inferring video windows with overlapping regions. By utilizing the predicted depth of the overlapping regions to compute an affine transform, predictions from one window can be aligned with those from another. Nevertheless, this method can introduce accumulated errors through successive affine alignments, leading to depth drift in extended videos. To address these challenges in ultra-long videos with a limited inference window size, we proposed key-frame referencing to inherit scale and shift information from past predictions and overlapping interpolation to ensure smooth inference across local windows.

\noindent{\bf Key-frame referencing.} As illustrated in ~\cref{fig:long_inference}, a subsequent video clip for inference is composed of three parts: $N-T_o-T_k$ future frames, $T_o$ overlapping frames from the previous clip and $T_k$ \textbf{key frames}. The key frames are subsampled from the previous frames with an interval of size $\Delta_k$. Therefore, the video clip to be consumed share the same length as during training. This approach incorporates content from earlier windows into the current window with minimal computation burden. Furthermore, we carefully select the values of $T_k$ and $\Delta_k$ to ensure that the first frame of a video is always positioned at the beginning of each clip, thereby enhancing depth consistency for extended videos. According to our experiment results, such simple strategy can significantly reduce accumulated scale drift, especially for long video.

\noindent{\bf Depth clip stitching.} Using $T_o$ overlapping frames (in ~\cref{fig:long_inference}) between two consecutive windows is crucial for avoiding flickering depth predictions. 
The effects of overlapping frames are twofold. 
First, by sharing partial frame features, the scale and shift across consecutive windows will be more similar. Second, the depth prediction for the overlapping frames is updated by interpolating between the two segments. Assume the depth for the $o_i$-th overlapping frame from the previous segment is denoted by $\mathbf{D}_{o_i}^{\text{pre}}$, and the depth from the current segment is denoted by $\mathbf{D}_{o_i}^{\text{cur}}$. The final depth is updated as $\mathbf{D}_{o_i} = \mathbf{D}_{o_i}^{\text{pre}} \cdot w_i + \mathbf{D}_{o_i}^{\text{cur}} \cdot (1 - w_i)$, where $w_i$ linearly decays from 1 to 0 as $i$ increases from 1 to $T_o$.