\section{Experiments}
\label{sec:experiments}
\input{tables/quant_video_depth_benchmark.tex}
\input{tables/quant_monocular_depth_benchmark.tex}


\subsection{Evaluation}
\noindent{\bf Datasets.} For the quantitative evaluation of video depth estimation, we utilize five datasets that encompass a wide range of scenes, including indoor~\cite{dai2017scannet,Silberman:ECCV12,palazzolo2019iros}, outdoor~\cite{geiger2013vision}, and wild environments~\cite{Butler_Wulff_Stanley_Black_2012}. Each video is evaluated using up to 500 frames, which is significantly more extensive than the 110 frames used in ~\cite{hu2024depthcrafter}. For results with 110 frames, see the appendix for details. In addition to video depth evaluation, we also assess our model's performance on static images~\cite{depth_anything_v2} on five image benchmarks ~\cite{geiger2013vision,Butler_Wulff_Stanley_Black_2012,Silberman:ECCV12,Schops_Schonberger_Galliani_Sattler_Schindler_Pollefeys_Geiger_2017,Igor_Nicholas_Zhang_Luo_Wang_Dai_Daniele_Mostajabi_Basart_Walter_et_al_2019}.
% \input{tables/video_eval_datasets.tex}

\noindent{\bf Metrics.} We evaluate our video depth model using both geometric accuracy and temporal stability metrics. In accordance with ~\cite{hu2024depthcrafter}, we first align the predicted depth maps with the ground truth by applying a uniform scale and shift throughout the video. For geometric accuracy, we compute the Absolute Relative Error (AbsRel) and $\delta_1$ metrics ~\cite{depth_anything_v2,hu2024depthcrafter}. To assess temporal stability, we use the Temporal Alignment Error (TAE) metric in ~\cite{depthanyvideo}, to measure the reprojection error of the depth maps between consecutive frames.

\subsection{Zero-shot Depth Estimation}
We compare our model against four representative video depth estimation models: NVDS~\cite{wang2023neural}, ChronoDepth~\cite{chronodepth}, DepthCrafter~\cite{hu2024depthcrafter}, and DepthAnyVideo~\cite{depthanyvideo} on established video depth benchmarks. 
Furthermore, we introduce two robust baselines, 1) Depth Anything V2~\cite{depth_anything_v2} (DAv2), and 2) NVDS + DAv2, \textit{i.e.}, replacing the base model in NVDS with DAv2. 
% DAv2~\cite{depth_anything_v2}  with a stabilization network in NVDS~\cite{wang2023neural}.
It is important to note that DepthAnyVideo~\cite{depthanyvideo} supports a maximum of 192 frames per video; therefore, we report metrics for the Sintel~\cite{Butler_Wulff_Stanley_Black_2012} dataset exclusively for this model, as other datasets contain videos that exceed this frame limit. For static image evaluation, we compare the performance of our model with DepthCrafter~\cite{hu2024depthcrafter}, DepthAnyVideo~\cite{depthanyvideo}, and Depth Anything V2~\cite{depth_anything_v2}.

% \noindent\paragraph{Video depth results.} 
\noindent\textbf{Video depth results.}
As demonstrated in Table~\ref{tab::quant_video_depth_benchmark}, our VDA model achieves state-of-the-art performance across all long video datasets, excelling in both geometric and temporal metrics. This underscores the effectiveness of our robust foundation model and the innovative design of our video model. Notably, on the KITTI~\cite{geiger2013vision}, Scannet~\cite{dai2017scannet}, and Bonn~\cite{palazzolo2019iros} datasets, our model surpasses other leading methods by a significant margin of approximately $10\%$ in the geometric accuracy metric $\delta_1$, although it is trained on much fewer video data compared to DepthCrafter~\cite{hu2024depthcrafter} (over 10 million frames) and DepthAnyVideo \cite{depthanyvideo} (6 million frames). For the short video synthetic dataset Sintel~\cite{Butler_Wulff_Stanley_Black_2012}, where sequences contain around 50 frames each, DepthCrafter~\cite{hu2024depthcrafter} exhibits better accuracy than our model. This discrepancy may be attributed to the absence of movie data, which features frames with focal lengths similar to those in Sintel~\cite{Butler_Wulff_Stanley_Black_2012}, in our model's training set. It is also worth highlighting that our compact model, VDA-S, which has significantly lower latency compared to other models (as shown in Table~\ref{tab::infer_time}), demonstrates superior geometric accuracy over representative diffusion-based methods for long videos.

\noindent\textbf{Image depth results.} The datasets in Table~\ref{tab::quant_monocular_depth_benchmark} consist of single images, identical to those utilized in DAv2~\cite{depth_anything_v2}. Although our model is designed for videos, it demonstrates the capability to handle static images. As shown in Table~\ref{tab::quant_monocular_depth_benchmark}, our video depth model achieves competitive depth metrics compared to DAv2-L~\cite{depth_anything_v2} across most datasets. This demonstrates that our model not only retains the geometric accuracy of the foundational model but also delivers high accuracy and consistency in video depth estimation.
%\input{figures/main_vis}

\noindent\textbf{Long video quantitative results.} We selected 10 scenes each from Bonn~\cite{palazzolo2019iros} and Scannet~\cite{dai2017scannet}, and 8 scenes from NYUv2~\cite{Silberman:ECCV12}, with each scene comprising 500 video frames. We then evaluated the video depth at frame lengths of 110, 192, 300, 400, and 500, where 110 and 192 correspond to the maximum window sizes of DepthCrafter~\cite{hu2024depthcrafter} and DepthAnyVideo~\cite{depthanyvideo}, respectively. The variation in metrics is shown in Figure~\ref{fig:long_video_decay}. As illustrated, our model significantly outperforms DepthCrafter~\cite{hu2024depthcrafter} in all evaluated frame lengths for all datasets, exhibiting minimal metric degradation as the number of frames increases. Furthermore, our model surpasses DepthAnyVideo~\cite{depthanyvideo} in Scannet~\cite{dai2017scannet} and NYUv2~\cite{Silberman:ECCV12}, and achieves comparable results in Bonn~\cite{palazzolo2019iros} for the 110 and 192 frame metrics. Most notably, our approach supports inference for arbitrarily long videos, providing a substantial advantage in practical applications.
\input{figures/long_video_decay.tex}

\noindent\textbf{Qualitative results.} We present two results of the long video visualization in~\ref{fig:long_video_show}. The second column represents image temporal profiles obtained by slicing images along the timeline at the red-line positions. The subsequent columns represent the corresponding depth profiles. The red boxes highlight instances where the depth profile of our model more closely resembles the ground truth (GT) compared to DepthCrafter~\cite{hu2024depthcrafter}, indicating superior geometric accuracy. Furthermore, our model demonstrates better temporal consistency, as shown in the blue boxes. In these instances, DepthCrafter~\cite{hu2024depthcrafter} exhibits drifted depth, and DAv2-L~\cite{depth_anything_v2} produces flickering depth. 
\input{figures/long_video_show.tex}

In addition to long videos, we present in-the-wild short video results in Figure~\ref{fig:short_video_show}. Depth Any Video~\cite{depthanyvideo} exhibits depth inconsistency even within a single reference window, as indicated by the blue boxes. Although DepthCrafter~\cite{hu2024depthcrafter} demonstrates smoother depth along video frames compared to Depth Any Video~\cite{depthanyvideo}, it fails to estimate accurate depth in some complex environments.
\input{figures/short_video_show.tex}


\noindent{\bf{Inference time.}} We measure the inference latency of various models on an A100 GPU. As shown in Table~\ref{tab::infer_time}, our large model achieves the lowest inference time compared to both diffusion-based methods (DepthAnyVideo~\cite{depthanyvideo} and DepthCrafter~\cite{hu2024depthcrafter}) and the transformer-based method (NVDS~\cite{wang2023neural}). This performance is attributed to our feed-forward transformer structure and lightweight temporal modules. Notably, the latency of our large model, VDA-L, is only approximately $10\%$ greater than that of DAv2-L~\cite{depth_anything_v2,hu2024depthcrafter}, which uses the same encoder structure, thus demonstrating the efficiency of our spatiotemporal head. Furthermore, the inference latency of our small model is less than 10ms, indicating its potential for real-time applications.
\input{tables/infer_time.tex}

\subsection{Ablation Studies}
Througout this section, we use the VDA-S model with a window size of 16, trained without image distillation unless otherwise specified. The metrics reported without a dataset name represent the mean values across all datasets.

\noindent{\bf{Temporal Loss.}} Temporal loss experiments are conducted on the TartanAir~\cite{wang2020tartanair} and VKITTI~\cite{cabon2020virtual} datasets. In addition to the TGM+SSI loss we proposed, we evaluate the performance of three other loss functions. The VideoAlign loss is a straightforward design that aligns predicted video depth to the ground truth using a shared scale-shift and computes the $l1$ loss. Building upon VideoAlign, the VideoAlign+SSI loss introduces an additional spatial loss to supervise the single-frame structure. The OPW+SSI loss combines optical flow-based warping loss proposed in~\cite{wang2023neural} with a single-frame spatial loss. SE refers to the stable error loss introduced in Equation~\ref{eq:stable_loss}. As shown in Table~\ref{tab::loss_ablation}, while VideoAlign and VideoAlign+SSI exhibit good geometric metrics, their video stability metrics are poor. Among loss functions with temporal constraints, our proposed TGM+SSI loss significantly outperforms the OPW+SSI loss on both geometric and stability metrics, and achieves metrics comparable to SE+SSI. It shows that TGM not only corrects the errors from OPW but also eliminates the dependency on optical flow.
\input{tables/loss_ablation.tex}

\noindent{\bf{Inference Strategies.}} To ablate our inference strategy, we consider four different inference schemes. \textbf{Baseline}, inference is performed independently on each window without overlapping frames. \textbf{Overlap Alignment (OA)}, based on scale-shift invariant alignment of the overlapping frames between two neighboring windows, this method stitches the two windows together. \textbf{Overlap Interpolation (OI)}, following the approach in DepthCrafter~\cite{hu2024depthcrafter}, this method splices two windows together after performing linear interpolation in the overlap region. \textbf{Overlap Interpolation + KeyFrame Reference (OI+KR)}, on the basis of OI, we additionally introduce key frames from the previous window as a reference for the current inference. As shown in Table~\ref{tab::infer_strategy}, OA achieves metrics comparable to those of OI+KR. However, it leads to cumulative scale drift during long video inference. This issue is illustrated in Figure~\ref{fig:infer_strategy_compare}, where we evaluate OA and OI+KR on an extended video with a duration of $4^\prime 04^{\prime \prime}$. Notably, the red boxed region in the last frame of the video processed by OA highlights a cumulative drift in the depth scale. In contrast, OI+KR maintains global scale consistency more effectively throughout the duration of the video. One possible explanation for the better metrics of OA in the evaluation datasets is that the 500-frame evaluation video dataset is not long enough to reflect the scale drift issues encountered in real-world, long-duration videos.

\noindent{\bf{Window sizes.}} As shown in Table~\ref{tab::infer_strategy}, the model with a window size of 32 exhibits better geometric accuracy and temporal consistency compared to the model with a window size of 16. However, increasing the window size beyond 32 does not yield additional benefits. Given that a larger window size requires more resources for both training and inference, we select a window size of 32 for our final model.

\input{tables/infer_strategy.tex}
\input{figures/infer_strategy_compare.tex}

\noindent{\bf{Training Strategies.}} In addition to training on synthetic datasets, we conduct an ablation study of distillation training strategies by incorporating an equal amount of pseudo-labeled real datasets. As shown in Table~\ref{tab::train_strategy_ablation}, the inclusion of real single-frame datasets in the distillation training process results in a notable enhancement of single-frame depth metrics in both AbsRel and $\delta_1$. Furthermore, it also improves video depth metrics.
\input{tables/train_strategy.tex}