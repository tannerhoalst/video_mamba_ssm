% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{deng2022nerf}
K.~Deng, A.~Liu, J.-Y. Zhu, and D.~Ramanan, ``Depth-supervised nerf: Fewer views and faster training for free,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 2022, pp. 12\,882--12\,891.

\bibitem{Zhou2019}
\BIBentryALTinterwordspacing
B.~Zhou, P.~Krähenbühl, and V.~Koltun, ``Does computer vision matter for action?'' \emph{Science Robotics}, vol.~4, 5 2019. [Online]. Available: \url{http://arxiv.org/abs/1905.12887 http://dx.doi.org/10.1126/scirobotics.aaw6661}
\BIBentrySTDinterwordspacing

\bibitem{dong2022depth4robotics}
X.~Dong, M.~A. Garratt, S.~G. Anavatti, and H.~A. Abbass, ``Towards real-time monocular depth estimation for robotics: A survey,'' \emph{IEEE Transactions on Intelligent Transportation Systems}, vol.~23, no.~10, pp. 16\,940--16\,961, 2022.

\bibitem{wang2019depth4vehicles}
Y.~Wang, W.-L. Chao, D.~Garg, B.~Hariharan, M.~Campbell, and K.~Q. Weinberger, ``Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 2019, pp. 8445--8453.

\bibitem{park2021dd3d}
D.~Park, R.~Ambrus, V.~Guizilini, J.~Li, and A.~Gaidon, ``Is pseudo-lidar needed for monocular 3d object detection?'' in \emph{IEEE/CVF International Conference on Computer Vision (ICCV)}, 2021.

\bibitem{Eigen2014}
\BIBentryALTinterwordspacing
D.~Eigen, C.~Puhrsch, and R.~Fergus, ``Depth map prediction from a single image using a multi-scale deep network,'' in \emph{Advances in Neural Information Processing Systems (NeurIPS)}, vol.~3.\hskip 1em plus 0.5em minus 0.4em\relax Neural information processing systems foundation, 6 2014, pp. 2366--2374. [Online]. Available: \url{https://arxiv.org/abs/1406.2283v1}
\BIBentrySTDinterwordspacing

\bibitem{Fu2018Dorn}
\BIBentryALTinterwordspacing
H.~Fu, M.~Gong, C.~Wang, K.~Batmanghelich, and D.~Tao, ``Deep ordinal regression network for monocular depth estimation,'' \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pp. 2002--2011, 6 2018. [Online]. Available: \url{https://arxiv.org/abs/1806.02446v1}
\BIBentrySTDinterwordspacing

\bibitem{Bhat2020adabins}
\BIBentryALTinterwordspacing
S.~F. Bhat, I.~Alhashim, and P.~Wonka, ``Adabins: Depth estimation using adaptive bins,'' \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pp. 4008--4017, 11 2020. [Online]. Available: \url{http://arxiv.org/abs/2011.14141 http://dx.doi.org/10.1109/CVPR46437.2021.00400}
\BIBentrySTDinterwordspacing

\bibitem{Ranftl2021dpt}
\BIBentryALTinterwordspacing
R.~Ranftl, A.~Bochkovskiy, and V.~Koltun, ``Vision transformers for dense prediction,'' \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)}, pp. 12\,159--12\,168, 3 2021. [Online]. Available: \url{https://arxiv.org/abs/2103.13413v1}
\BIBentrySTDinterwordspacing

\bibitem{Patil2022p3depth}
\BIBentryALTinterwordspacing
V.~Patil, C.~Sakaridis, A.~Liniger, and L.~V. Gool, ``{P3Depth}: Monocular depth estimation with a piecewise planarity prior,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}.\hskip 1em plus 0.5em minus 0.4em\relax {IEEE}, 2022, pp. 1600--1611. [Online]. Available: \url{https://doi.org/10.1109/CVPR52688.2022.00166}
\BIBentrySTDinterwordspacing

\bibitem{Yuan2022newcrf}
\BIBentryALTinterwordspacing
W.~Yuan, X.~Gu, Z.~Dai, S.~Zhu, and P.~Tan, ``Neural window fully-connected crfs for monocular depth estimation,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}.\hskip 1em plus 0.5em minus 0.4em\relax {IEEE}, 2022, pp. 3906--3915. [Online]. Available: \url{https://doi.org/10.1109/CVPR52688.2022.00389}
\BIBentrySTDinterwordspacing

\bibitem{piccinelli2023idisc}
L.~Piccinelli, C.~Sakaridis, and F.~Yu, ``{iDisc}: Internal discretization for monocular depth estimation,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 2023.

\bibitem{Wang2020traingermany}
\BIBentryALTinterwordspacing
Y.~Wang, X.~Chen, Y.~You, L.~E. Li, B.~Hariharan, M.~Campbell, K.~Q. Weinberger, and W.~L. Chao, ``Train in germany, test in the usa: Making 3d object detectors generalize,'' \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pp. 11\,710--11\,720, 5 2020. [Online]. Available: \url{https://arxiv.org/abs/2005.08139v1}
\BIBentrySTDinterwordspacing

\bibitem{yin2023metric3d}
W.~Yin, C.~Zhang, H.~Chen, Z.~Cai, G.~Yu, K.~Wang, X.~Chen, and C.~Shen, ``Metric3d: Towards zero-shot metric 3d prediction from a single image,'' in \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)}, 2023, pp. 9043--9053.

\bibitem{guizilini2023zerodepth}
V.~Guizilini, I.~Vasiljevic, D.~Chen, R.~Ambruș, and A.~Gaidon, ``Towards zero-shot scale-aware monocular depth estimation,'' in \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)}, 2023, pp. 9233--9243.

\bibitem{hu2024metric3dv2}
M.~Hu, W.~Yin, C.~Zhang, Z.~Cai, X.~Long, H.~Chen, K.~Wang, G.~Yu, C.~Shen, and S.~Shen, ``Metric3d v2: A versatile monocular geometric foundation model for zero-shot metric depth and surface normal estimation,'' \emph{arXiv preprint arXiv:2404.15506}, 2024.

\bibitem{piccinelli2024unidepth}
L.~Piccinelli, Y.-H. Yang, C.~Sakaridis, M.~Segu, S.~Li, L.~Van~Gool, and F.~Yu, ``Unidepth: Universal monocular metric depth estimation,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 2024, pp. 10\,106--10\,116.

\bibitem{bonzanini2021perception}
A.~D. Bonzanini, A.~Mesbah, and S.~Di~Cairano, ``Perception-aware chance-constrained model predictive control for uncertain environments,'' in \emph{2021 American Control Conference (ACC)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2021, pp. 2082--2087.

\bibitem{mesbah2016stochastic}
A.~Mesbah, ``Stochastic model predictive control: An overview and perspectives for future research,'' \emph{IEEE Control Systems Magazine}, vol.~36, no.~6, pp. 30--44, 2016.

\bibitem{yang2023safe}
S.~Yang, G.~J. Pappas, R.~Mangharam, and L.~Lindemann, ``Safe perception-based control under stochastic sensor uncertainty using conformal prediction,'' \emph{arXiv preprint arXiv:2304.00194}, 2023.

\bibitem{bemporad2007robust}
A.~Bemporad and M.~Morari, ``Robust model predictive control: A survey,'' in \emph{Robustness in identification and control}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2007, pp. 207--226.

\bibitem{silberman2012nyu}
P.~K. Nathan~Silberman, Derek~Hoiem and R.~Fergus, ``Indoor segmentation and support inference from rgbd images,'' in \emph{The European Conference on Computer Vision (ECCV)}, 2012.

\bibitem{Geiger2012kitti}
A.~Geiger, P.~Lenz, and R.~Urtasun, ``Are we ready for autonomous driving? {The} {KITTI} vision benchmark suite,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 2012.

\bibitem{ranftl2020midas}
R.~Ranftl, K.~Lasinger, D.~Hafner, K.~Schindler, and V.~Koltun, ``Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer,'' \emph{IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)}, vol.~44, no.~3, pp. 1623--1637, 2020.

\bibitem{eftekhar2021omnidata}
A.~Eftekhar, A.~Sax, J.~Malik, and A.~Zamir, ``Omnidata: A scalable pipeline for making multi-task mid-level vision datasets from 3d scans,'' in \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)}, 2021, pp. 10\,786--10\,796.

\bibitem{yin2021leres}
W.~Yin, J.~Zhang, O.~Wang, S.~Niklaus, L.~Mai, S.~Chen, and C.~Shen, ``Learning to recover 3d scene shape from a single image,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 2021, pp. 204--213.

\bibitem{ke2024marigold}
B.~Ke, A.~Obukhov, S.~Huang, N.~Metzger, R.~C. Daudt, and K.~Schindler, ``Repurposing diffusion-based image generators for monocular depth estimation,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 2024, pp. 9492--9502.

\bibitem{yang2024da1}
L.~Yang, B.~Kang, Z.~Huang, X.~Xu, J.~Feng, and H.~Zhao, ``Depth anything: Unleashing the power of large-scale unlabeled data,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 2024, pp. 10\,371--10\,381.

\bibitem{yang2024da2}
L.~Yang, B.~Kang, Z.~Huang, Z.~Zhao, X.~Xu, J.~Feng, and H.~Zhao, ``Depth anything v2,'' \emph{arXiv preprint arXiv:2406.09414}, 2024.

\bibitem{Laina2016}
\BIBentryALTinterwordspacing
I.~Laina, C.~Rupprecht, V.~Belagiannis, F.~Tombari, and N.~Navab, ``Deeper depth prediction with fully convolutional residual networks,'' \emph{Proceedings of the International Conference on 3D Vision (3DV)}, pp. 239--248, 6 2016. [Online]. Available: \url{https://arxiv.org/abs/1606.00373v2}
\BIBentrySTDinterwordspacing

\bibitem{Liu2015}
\BIBentryALTinterwordspacing
F.~Liu, C.~Shen, G.~Lin, and I.~Reid, ``Learning depth from single monocular images using deep convolutional neural fields,'' \emph{IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)}, vol.~38, pp. 2024--2039, 2 2015. [Online]. Available: \url{http://arxiv.org/abs/1502.07411 http://dx.doi.org/10.1109/TPAMI.2015.2505283}
\BIBentrySTDinterwordspacing

\bibitem{Yang2021}
\BIBentryALTinterwordspacing
G.~Yang, H.~Tang, M.~Ding, N.~Sebe, and E.~Ricci, ``Transformer-based attention networks for continuous pixel-wise prediction,'' \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)}, pp. 16\,249--16\,259, 3 2021. [Online]. Available: \url{https://arxiv.org/abs/2103.12091v2}
\BIBentrySTDinterwordspacing

\bibitem{bhat2023zoedepth}
S.~F. Bhat, R.~Birkl, D.~Wofk, P.~Wonka, and M.~M{\"u}ller, ``Zoedepth: Zero-shot transfer by combining relative and metric depth,'' \emph{arXiv preprint arXiv:2302.12288}, 2023.

\bibitem{facil2019camconvs}
J.~M. Facil, B.~Ummenhofer, H.~Zhou, L.~Montesano, T.~Brox, and J.~Civera, ``Cam-convs: Camera-aware multi-scale convolutions for single-view depth,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 2019, pp. 11\,826--11\,835.

\bibitem{Lee2019bts}
\BIBentryALTinterwordspacing
J.~H. Lee, M.~Han, D.~W. Ko, and I.~H. Suh, ``From big to small: Multi-scale local planar guidance for monocular depth estimation,'' \emph{CoRR}, vol. abs/1907.10326, 7 2019. [Online]. Available: \url{http://arxiv.org/abs/1907.10326}
\BIBentrySTDinterwordspacing

\bibitem{Lopez2020mapillary}
M.~L. Antequera, P.~Gargallo, M.~Hofinger, S.~R. Bul{\`o}, Y.~Kuang, and P.~Kontschieder, ``Mapillary planet-scale depth dataset,'' in \emph{The European Conference on Computer Vision (ECCV)}.\hskip 1em plus 0.5em minus 0.4em\relax Springer International Publishing, 2020, pp. 589--604.

\bibitem{bochkovskii2024depthpro}
A.~Bochkovskii, A.~Delaunoy, H.~Germain, M.~Santos, Y.~Zhou, S.~R. Richter, and V.~Koltun, ``Depth pro: Sharp monocular metric depth in less than a second,'' \emph{arXiv preprint arXiv:2410.02073}, 2024.

\bibitem{Dosovitskiy2020VIT}
\BIBentryALTinterwordspacing
A.~Dosovitskiy, L.~Beyer, A.~Kolesnikov, D.~Weissenborn, X.~Zhai, T.~Unterthiner, M.~Dehghani, M.~Minderer, G.~Heigold, S.~Gelly, J.~Uszkoreit, and N.~Houlsby, ``An image is worth 16x16 words: Transformers for image recognition at scale,'' in \emph{International Conference on Learning Representations (ICLR)}.\hskip 1em plus 0.5em minus 0.4em\relax OpenReview.net, 2021. [Online]. Available: \url{https://openreview.net/forum?id=YicbFdNTTy}
\BIBentrySTDinterwordspacing

\bibitem{geyer2020a2d2}
\BIBentryALTinterwordspacing
J.~Geyer, Y.~Kassahun, M.~Mahmudi, X.~Ricou, R.~Durgesh, A.~S. Chung, L.~Hauswald, V.~H. Pham, M.~M{\"u}hlegg, S.~Dorn, T.~Fernandez, M.~J{\"a}nicke, S.~Mirashi, C.~Savani, M.~Sturm, O.~Vorobiov, M.~Oelker, S.~Garreis, and P.~Schuberth, ``{A2D2: Audi Autonomous Driving Dataset},'' \emph{arXiv preprint arXiv:2004.06320}, 2020. [Online]. Available: \url{https://www.a2d2.audi}
\BIBentrySTDinterwordspacing

\bibitem{2021argoverse2}
B.~Wilson, W.~Qi, T.~Agarwal, J.~Lambert, J.~Singh, S.~Khandelwal, B.~Pan, R.~Kumar, A.~Hartnett, J.~K. Pontes, D.~Ramanan, P.~Carr, and J.~Hays, ``Argoverse 2: Next generation datasets for self-driving perception and forecasting,'' in \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem{baruch2021arkitscenes}
\BIBentryALTinterwordspacing
G.~Baruch, Z.~Chen, A.~Dehghan, T.~Dimry, Y.~Feigin, P.~Fu, T.~Gebauer, B.~Joffe, D.~Kurz, A.~Schwartz, and E.~Shulman, ``{ARK}itscenes - a diverse real-world dataset for 3d indoor scene understanding using mobile {RGB}-d data,'' in \emph{Advances in Neural Information Processing Systems (NIPS)}, 2021. [Online]. Available: \url{https://openreview.net/forum?id=tjZjv_qh_CE}
\BIBentrySTDinterwordspacing

\bibitem{black2023bedlam}
M.~J. Black, P.~Patel, J.~Tesch, and J.~Yang, ``{BEDLAM}: A synthetic dataset of bodies exhibiting detailed lifelike animated motion,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 2023, pp. 8726--8737.

\bibitem{yao2020blendedmvs}
Y.~Yao, Z.~Luo, S.~Li, J.~Zhang, Y.~Ren, L.~Zhou, T.~Fang, and L.~Quan, ``Blendedmvs: A large-scale dataset for generalized multi-view stereo networks,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 2020, pp. 1790--1799.

\bibitem{ling2024dl3dv}
L.~Ling, Y.~Sheng, Z.~Tu, W.~Zhao, C.~Xin, K.~Wan, L.~Yu, Q.~Guo, Z.~Yu, Y.~Lu \emph{et~al.}, ``{DL3DV}-10k: A large-scale scene dataset for deep learning-based 3d vision,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 2024, pp. 22\,160--22\,169.

\bibitem{yang2019drivingstereo}
G.~Yang, X.~Song, C.~Huang, Z.~Deng, J.~Shi, and B.~Zhou, ``Drivingstereo: A large-scale dataset for stereo matching in autonomous driving scenarios,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 2019.

\bibitem{karaev2023dynamicreplica}
N.~Karaev, I.~Rocco, B.~Graham, N.~Neverova, A.~Vedaldi, and C.~Rupprecht, ``Dynamicstereo: Consistent dynamic depth from stereo videos,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 2023.

\bibitem{le2021eden}
H.-A. Le, T.~Mensink, P.~Das, S.~Karaoglu, and T.~Gevers, ``Eden: Multimodal synthetic dataset of enclosed garden scenes,'' in \emph{Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)}, 2021, pp. 1579--1589.

\bibitem{liu2022hoi4d}
Y.~Liu, Y.~Liu, C.~Jiang, K.~Lyu, W.~Wan, H.~Shen, B.~Liang, Z.~Fu, H.~Wang, and L.~Yi, ``Hoi4d: A 4d egocentric dataset for category-level human-object interaction,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 2022, pp. 21\,013--21\,022.

\bibitem{ramakrishnan2021habitat}
\BIBentryALTinterwordspacing
S.~K. Ramakrishnan, A.~Gokaslan, E.~Wijmans, O.~Maksymets, A.~Clegg, J.~M. Turner, E.~Undersander, W.~Galuba, A.~Westbury, A.~X. Chang, M.~Savva, Y.~Zhao, and D.~Batra, ``Habitat-matterport 3d dataset ({HM}3d): 1000 large-scale 3d environments for embodied {AI},'' in \emph{Advances in Neural Information Processing Systems (NIPS)}, 2021. [Online]. Available: \url{https://arxiv.org/abs/2109.08238}
\BIBentrySTDinterwordspacing

\bibitem{chang2017matterport3d}
A.~Chang, A.~Dai, T.~Funkhouser, M.~Halber, M.~Niessner, M.~Savva, S.~Song, A.~Zeng, and Y.~Zhang, ``Matterport3d: Learning from rgb-d data in indoor environments,'' in \emph{Proceedings of the International Conference on 3D Vision (3DV)}, 2017.

\bibitem{li2023matrixcity}
Y.~Li, L.~Jiang, L.~Xu, Y.~Xiangli, Z.~Wang, D.~Lin, and B.~Dai, ``Matrixcity: A large-scale city dataset for city-scale neural rendering and beyond,'' in \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)}, 2023, pp. 3205--3215.

\bibitem{li2018megadepth}
Z.~Li and N.~Snavely, ``Megadepth: Learning single-view depth prediction from internet photos,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 2018, pp. 2041--2050.

\bibitem{arnold2022mapfree}
E.~Arnold, J.~Wynn, S.~Vicente, G.~Garcia-Hernando, {\'{A}}.~Monszpart, V.~A. Prisacariu, D.~Turmukhambetov, and E.~Brachmann, ``Map-free visual relocalization: Metric pose relative to a single image,'' in \emph{European Conference on Computer Vision (ECCV)}, 2022.

\bibitem{zheng2023pointodyssey}
Y.~Zheng, A.~W. Harley, B.~Shen, G.~Wetzstein, and L.~J. Guibas, ``Pointodyssey: A large-scale synthetic dataset for long-term point tracking,'' in \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)}, 2023, pp. 19\,855--19\,865.

\bibitem{dai2017scannet}
A.~Dai, A.~X. Chang, M.~Savva, M.~Halber, T.~Funkhouser, and M.~Nie{\ss}ner, ``Scannet: Richly-annotated 3d reconstructions of indoor scenes,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 2017.

\bibitem{yeshwanthliu2023scannetpp}
C.~Yeshwanth, Y.-C. Liu, M.~Nie{\ss}ner, and A.~Dai, ``Scannet++: A high-fidelity dataset of 3d indoor scenes,'' in \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)}, 2023.

\bibitem{wang2020tartanair}
W.~Wang, D.~Zhu, X.~Wang, Y.~Hu, Y.~Qiu, C.~Wang, Y.~Hu, A.~Kapoor, and S.~Scherer, ``Tartanair: A dataset to push the limits of visual slam,'' in \emph{2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2020, pp. 4909--4916.

\bibitem{zamir2018taskonomy}
A.~R. Zamir, A.~Sax, W.~B. Shen, L.~Guibas, J.~Malik, and S.~Savarese, ``Taskonomy: Disentangling task transfer learning,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2018.

\bibitem{sun2020waymo}
P.~Sun, H.~Kretzschmar, X.~Dotiwalla, A.~Chouard, V.~Patnaik, P.~Tsui, J.~Guo, Y.~Zhou, Y.~Chai, B.~Caine \emph{et~al.}, ``Scalability in perception for autonomous driving: Waymo open dataset,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 2020, pp. 2446--2454.

\bibitem{xia2024wildrgbd}
H.~Xia, Y.~Fu, S.~Liu, and X.~Wang, ``Rgbd objects in the wild: Scaling real-world 3d object learning from rgb-d videos,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 2024, pp. 22\,378--22\,389.

\bibitem{Song2015sunrgbd}
S.~Song, S.~P. Lichtenberg, and J.~Xiao, ``Sun rgb-d: A rgb-d scene understanding benchmark suite,'' \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, vol. 07-12-June-2015, pp. 567--576, 10 2015.

\bibitem{koch2022ibims}
T.~Koch, L.~Liebel, M.~Körner, and F.~Fraundorfer, ``Comparison of monocular depth estimation methods using geometrically relevant metrics on the {IBims-1} dataset,'' \emph{Computer Vision and Image Understanding (CVIU)}, vol. 191, p. 102877, 2020.

\bibitem{sturm12tumrgbd}
J.~Sturm, N.~Engelhard, F.~Endres, W.~Burgard, and D.~Cremers, ``A benchmark for the evaluation of rgb-d slam systems,'' in \emph{Proc. of the International Conference on Intelligent Robot Systems (IROS)}, 2012.

\bibitem{jung2022hammer}
H.~Jung, P.~Ruhkamp, G.~Zhai, N.~Brasch, Y.~Li, Y.~Verdie, J.~Song, Y.~Zhou, A.~Armagan, S.~Ilic \emph{et~al.}, ``Is my depth ground-truth good enough? {HAMMER} -- {H}ighly {A}ccurate {M}ulti-{M}odal dataset for d{E}nse {3D} scene {R}egression,'' \emph{arXiv preprint arXiv:2205.04565}, 2022.

\bibitem{schoeps2017eth3d}
T.~Sch\"ops, J.~L. Sch\"onberger, S.~Galliani, T.~Sattler, K.~Schindler, M.~Pollefeys, and A.~Geiger, ``A multi-view stereo benchmark with high-resolution images and multi-camera videos,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 2017.

\bibitem{Butler2012sintel}
D.~J. Butler, J.~Wulff, G.~B. Stanley, and M.~J. Black, ``A naturalistic open source movie for optical flow evaluation,'' in \emph{The European Conference on Computer Vision (ECCV)}, ser. Part IV, LNCS 7577.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2012, pp. 611--625.

\bibitem{Guizilini2020ddad}
V.~Guizilini, R.~Ambrus, S.~Pillai, A.~Raventos, and A.~Gaidon, ``3d packing for self-supervised monocular depth estimation,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 2020.

\bibitem{nuscenes}
H.~Caesar, V.~Bankiti, A.~H. Lang, S.~Vora, V.~E. Liong, Q.~Xu, A.~Krishnan, Y.~Pan, G.~Baldan, and O.~Beijbom, ``nuscenes: A multimodal dataset for autonomous driving,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 2020.

\bibitem{ornek20222metrics}
E.~P. {\"O}rnek, S.~Mudgal, J.~Wald, Y.~Wang, N.~Navab, and F.~Tombari, ``From 2d to 3d: Re-thinking benchmarking of monocular depth prediction,'' \emph{arXiv preprint arXiv:2203.08122}, 2022.

\bibitem{pytorch}
A.~Paszke, S.~Gross, F.~Massa, A.~Lerer, J.~Bradbury, G.~Chanan, T.~Killeen, Z.~Lin, N.~Gimelshein, L.~Antiga, A.~Desmaison, A.~Kopf, E.~Yang, Z.~DeVito, M.~Raison, A.~Tejani, S.~Chilamkurthy, B.~Steiner, L.~Fang, J.~Bai, and S.~Chintala, ``Pytorch: An imperative style, high-performance deep learning library,'' in \emph{Advances in Neural Information Processing Systems (NeurIPS)}.\hskip 1em plus 0.5em minus 0.4em\relax Curran Associates, Inc., 2019, pp. 8024--8035.

\bibitem{nickolls2008cuda}
J.~Nickolls, I.~Buck, M.~Garland, and K.~Skadron, ``Scalable parallel programming with cuda: Is cuda the parallel programming model that application developers have been waiting for?'' \emph{Queue}, vol.~6, no.~2, pp. 40--53, 2008.

\bibitem{Loshchilov2017adamw}
\BIBentryALTinterwordspacing
I.~Loshchilov and F.~Hutter, ``Decoupled weight decay regularization,'' \emph{7th International Conference on Learning Representations, ICLR 2019}, 11 2017. [Online]. Available: \url{https://arxiv.org/abs/1711.05101v3}
\BIBentrySTDinterwordspacing

\bibitem{oquab2023dinov2}
M.~Oquab, T.~Darcet, T.~Moutakanni, H.~Vo, M.~Szafraniec, V.~Khalidov, P.~Fernandez, D.~Haziza, F.~Massa, A.~El-Nouby \emph{et~al.}, ``Dinov2: Learning robust visual features without supervision,'' \emph{arXiv preprint arXiv:2304.07193}, 2023.

\bibitem{leroy2024master}
V.~Leroy, Y.~Cabon, and J.~Revaud, ``Grounding image matching in 3d with mast3r,'' \emph{arXiv preprint arXiv:2406.09756}, 2024.

\end{thebibliography}
