codex --model=gpt-5.1-codex \
  --dangerously-bypass-approvals-and-sandbox \
  --enable web_search_request \
  -c model_reasoning_effort="high"



uv pip install --upgrade --force-reinstall \
    torch==2.9.1+cu130 torchvision==0.24.1+cu130 torchaudio==2.9.1+cu130 \
    --index-url https://download.pytorch.org/whl/cu130


  Conceptually, the idea is sound: replacing quadratic temporal
  attention with linear-cost SSMs, keeping the heavy Depth Anything
  encoder frozen, and layering in practical safeguards (motion-
  based masking, EMA stabilization, keyframe resets). Nothing jumps
  out as impossible. The main risks are the unvalidated assumptions
  —e.g., how well per-pixel Mamba scans handle very long horizons in
  practice, whether the teacher-student SE supervision captures enough
  temporal nuance, and the compute trade-off if the encoder remains the
  bottleneck. You’ll still need to implement and benchmark it to prove
  it “works,” but as a design blueprint, it’s logically consistent and
  outlines the right mitigation strategies for known pitfalls.


python3 -m venv .venv
source .venv/bin/activate
CC="cc -mavx2" uv pip install -r UniDepth/requirements.txt --upgrade --force-reinstall

python -m unidepth.models.unidepthv2.inference \
  --image /path/to/frame.png \
  --model-id lpiccinelli/unidepth-v2-vits14 \
  --save-depth outputs/depth.npy \
  --save-points outputs/points.npy