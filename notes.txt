codex --model=gpt-5.1-codex \
  --dangerously-bypass-approvals-and-sandbox \
  --enable web_search_request \
  -c model_reasoning_effort="high"



  Conceptually, the idea is sound: replacing quadratic temporal
  attention with linear-cost SSMs, keeping the heavy Depth Anything
  encoder frozen, and layering in practical safeguards (motion-
  based masking, EMA stabilization, keyframe resets). Nothing jumps
  out as impossible. The main risks are the unvalidated assumptions
  —e.g., how well per-pixel Mamba scans handle very long horizons in
  practice, whether the teacher-student SE supervision captures enough
  temporal nuance, and the compute trade-off if the encoder remains the
  bottleneck. You’ll still need to implement and benchmark it to prove
  it “works,” but as a design blueprint, it’s logically consistent and
  outlines the right mitigation strategies for known pitfalls.