codex --model=gpt-5.1-codex-max \
  --dangerously-bypass-approvals-and-sandbox \
  --enable web_search_request \
  -c model_reasoning_effort="high"


  Conceptually, the idea is sound: replacing quadratic temporal
  attention with linear-cost SSMs, keeping the heavy Depth Anything
  encoder frozen, and layering in practical safeguards (motion-
  based masking, EMA stabilization, keyframe resets). Nothing jumps
  out as impossible. The main risks are the unvalidated assumptions
  —e.g., how well per-pixel Mamba scans handle very long horizons in
  practice, whether the teacher-student SE supervision captures enough
  temporal nuance, and the compute trade-off if the encoder remains the
  bottleneck. You’ll still need to implement and benchmark it to prove
  it “works,” but as a design blueprint, it’s logically consistent and
  outlines the right mitigation strategies for known pitfalls.




sudo apt-get update
sudo apt-get install -y libjpeg-dev zlib1g-dev libopenjp2-7-dev libtiff-dev libwebp-dev liblcms2-dev
sudo apt-get install -y build-essential cmake ninja-build

python3 -m venv .venv
source .venv/bin/activate
CC="cc -mavx2" uv pip install --extra-index-url https://download.pytorch.org/whl/cu130 -r UniDepth/requirements.txt --upgrade --force-reinstall
uv pip install -v --no-build-isolation -U git+https://github.com/facebookresearch/xformers.git@main#egg=xformers
uv pip install -e UniDepth

cd UniDepth/unidepth/ops/knn
TORCH_CUDA_ARCH_LIST="8.0 8.6 8.9 9.0+PTX" python setup.py build install
cd ../extract_patches
TORCH_CUDA_ARCH_LIST="8.0 8.6 8.9 9.0+PTX" python setup.py build install
cd ../../../..

python -m unidepth.models.unidepthv2.inference \
  --image /home/thoalst/Pictures/Screenshots/ufc.png \
  --model-id lpiccinelli/unidepth-v2-vitb14 \
  --save-depth /mnt/vrdata/depth_maps/unidepth/ufc.npy


python visualize_depth.py /mnt/vrdata/depth_maps/unidepth/savannah_4.npy


python benchmark_unidepth_inprocess.py --image /home/thoalst/Pictures/Screenshots/ufc.png --runs 60 --warmup 3 --model-id lpiccinelli/unidepth-v2-vitb14

  - defaults: 5 warm-up runs, 60 timed runs, image /home/thoalst/Pictures/Screenshots/savannah.png, model lpiccinelli/unidepth-v2-vits14, device auto (CUDA if available).
Common options:
  - --runs 100 to change the timed loop count.
  - --warmup 2 to adjust warm-ups.
  - --image /path/to/img.png to test another image.
  - --device cpu to force CPU.
  - --save-depth /mnt/vrdata/depth_maps/unidepth/savannah_loop.npy to save the depth from the last timed run only (avoids I/O in the loop).
  - --verbose to print per-run timings.